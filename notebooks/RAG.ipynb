{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "53a43b03-efe3-498d-8463-9ae52c425912",
   "metadata": {},
   "source": [
    "# TD: RAG\n",
    "\n",
    "Dans ce notebook, un RAG basique est implémenté:\n",
    "- On chunk les documents par paragraphes\n",
    "- On a un embedding pour les chunks\n",
    "- Pour une question, on peut embedde la question et récupérer les N chunks les plus pertinents\n",
    "- On utilise un modèle de génération de texte (SMoLL) pour faire la partie question + chunks les plus pertinents -> réponse.\n",
    "\n",
    "Téléchargez (cette archive)[https://drive.google.com/file/d/1TnfKs7bTwmpbXklbgiIBpdw7I_wJ5y9Y/view?usp=sharing] avec différentes \n",
    "\n",
    "Dans ce TD, vous allez expérimenter différentes façons de chunk et d'embeded les documents et les questions pour que le RAG retrieve les documents les plus pertinents. <br/>\n",
    "Vous expérimenterez aussi la prompt donnée au générateur de texte pour avoir les meilleures réponses.\n",
    "\n",
    "Voici la [liste de questions](https://drive.google.com/file/d/14hZ0hTx5dM1WgJYewZsn9BkHzEReq-pj/view?usp=sharing) que je poserai au RAG. </br>\n",
    "A rendre: \n",
    "- Le notebook de votre RAG\n",
    "- un CSV avec question,embedding,rag_reply\n",
    "- un CSV avec chunk,embedding</br>\n",
    "L'embedding doit être le JSON d'une liste de float.</br>\n",
    "Quand je ferai \"json.loads(embedding)\", je dois récupérer une liste de floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349cd335-0b3d-45cd-873f-3890d85413a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0747a5ba-a9b7-4932-a254-7d05f4c412ab",
   "metadata": {},
   "source": [
    "# Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824ec18e-fdad-4093-a9bd-7b226d14942a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"../data/raw/rag/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37fe1e46-e7ba-42d4-af4a-72f19ee39548",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = []\n",
    "for filename in path.glob(\"*.md\"):\n",
    "    with open(filename) as f:\n",
    "        texts.append(f.read())\n",
    "\n",
    "texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66344eb7-0679-4ea5-aa1b-ee3c84f8c98c",
   "metadata": {},
   "source": [
    "# Chunk\n",
    "## Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd5aee6-83de-4d1a-a0bf-15a28cf3cc94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_class(text):\n",
    "    chunks = text.split(\"\\n\\n\")\n",
    "    title = chunks[0].replace(\"# Title: \", \"\")\n",
    "    return {\"title\": title, \"chunks\": chunks}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c937dc-674f-4ff7-8a76-b87d2d23c675",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_class_add_title(text):\n",
    "    chunks = text.split(\"\\n\\n\")\n",
    "    title = chunks[0].replace(\"# Title: \", \"\")\n",
    "    return {\"title\": title, \"chunks\": [f\"{title}: {chunk}\" for chunk in chunks]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519b2380-54b3-44b1-a3fd-27f97a324c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = sum((parse_class_add_title(txt)[\"chunks\"] for txt in texts), [])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104688fd-ef0e-469b-bd08-c435dd4ebdea",
   "metadata": {},
   "source": [
    "# Embedding\n",
    "\n",
    "## BAAI's embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6546cc1c-b08e-4a84-9439-db52f6b3b281",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlagEmbedding import FlagModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8778ce83-5d6b-48d7-8871-a90316414699",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FlagModel(\n",
    "    'BAAI/bge-base-en-v1.5',\n",
    "    query_instruction_for_retrieval=\"Represent this sentence for searching relevant passages:\",\n",
    "    use_fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ac8413-e373-4d6e-adec-dc62113f36ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_embedding = model.encode(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e8c758-b27e-4779-b9e4-4c075927547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "    \"Who is the reinforcement learning teacher?\",\n",
    "    \"In what class will I learn game AI?\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f7c1c3-127a-47d9-a49f-18dab8968d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = model.encode(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3150b5-63fa-4b07-a9c7-66ef56c4ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_scores = query_embedding @ corpus_embedding.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d0738f-cefc-458b-aaa5-2a096e4fc739",
   "metadata": {},
   "outputs": [],
   "source": [
    "for query, score in zip(queries, sim_scores):\n",
    "    print(\" ---- \")\n",
    "    print(\"Query: \", query)\n",
    "    indexes = np.argsort(score)[-5:]\n",
    "    print(\"Sources:\")\n",
    "    for i, idx in enumerate(reversed(indexes)):\n",
    "        if score[idx] > .5:\n",
    "            print(f\"{i+1} -- similarity {score[idx]:.2f} -- \\\"\", chunks[idx], '\"')\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186611b2-d128-4884-9bec-a6935715e371",
   "metadata": {},
   "source": [
    "# Eval retrieval: Mean Reciprocal Rank\n",
    "Le fichier [question_answer_short.csv](https://drive.google.com/file/d/1EB8IwGlqvpNy3oq7xyR2IzdqJDX8C_fr/view?usp=drive_link) contient une liste de question et le texte à retrouver dans les documents.<br/>\n",
    "Je considère que tout chunk contenant le \"texte à retrouver\" était un bon chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd74624-9c0b-4480-b572-02cae5e84dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path / \"question_answer_short.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb81d11-888f-400e-b811-54ef53da5d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_embedding = model.encode(list(df[\"question\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136a9728-1a69-433a-b425-887de3b6f556",
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptable_chunks = []\n",
    "for answer in df[\"answer\"]:\n",
    "    chunks_ok = set(i for i, chunk in enumerate(chunks) if answer in chunk)\n",
    "    acceptable_chunks.append(chunks_ok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31efd406-3209-4714-a1a9-ed4c291d614a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mrr(sim_score, acceptable_chunks):\n",
    "    ranks = []\n",
    "    for this_score, this_acceptable_chunks in zip(sim_score, acceptable_chunks):\n",
    "        indexes = reversed(np.argsort(this_score))\n",
    "        rank = 1 + next(i for i, idx in enumerate(indexes) if idx in this_acceptable_chunks)\n",
    "        ranks.append(rank)\n",
    "        \n",
    "    return {\n",
    "        \"score\": sum(1 / r if r < 6 else 0 for r in ranks) / len(ranks),\n",
    "        \"ranks\": ranks,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf24857a-ee28-42d7-8e14-9a16db07a9c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_scores = query_embedding @ corpus_embedding.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1929a6-6d5c-4c26-9c05-3a2ca2d461f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = compute_mrr(sim_scores, acceptable_chunks)\n",
    "res[\"score\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549268e7-8bf9-47a5-b9ac-64decbb88fa5",
   "metadata": {},
   "source": [
    "# Text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a534b4-8011-46d8-9a56-6f4ed10c22d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_context(query, corpus, corpus_embeddings):\n",
    "    query_embedding = model.encode([query])\n",
    "    sim_scores = query_embedding @ corpus_embedding.T\n",
    "    indexes = list(np.argsort(sim_scores[0]))[-5:]\n",
    "    return [corpus[i] for i in indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb22e930-351f-450a-80a7-72dfa9f721a6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_context(\"Which class will teach me to build a chatbot?\", chunks, corpus_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9181f90f-c7e0-450f-bb87-f2002c061107",
   "metadata": {},
   "source": [
    "## SMOLL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1de9b848-78ea-4cc3-9aa4-88732d871b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "checkpoint = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\n",
    "# checkpoint = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "# checkpoint = \"amd/Instella-3B\"\n",
    "\n",
    "device = \"cpu\" # for GPU usage or \"cpu\" for CPU usage\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model_generator = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edd1fc8-d9f6-4c55-a2f9-2f2952280471",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_smoll_prompt(query, corpus, corpus_embedding):\n",
    "    context_str = \"\\n\\n\".join(get_context(query, chunks, corpus_embedding))\n",
    "\n",
    "    prompt = f\"\"\"<|im_start|>system\n",
    "You reply to the user's request using only context information.\n",
    "Context information to answer \"{query}\" is below\n",
    "------\n",
    "Context:\n",
    "{context_str}\n",
    "------\n",
    "You are a helpful assistant for a Computer Science university. You reply to students'questions about the courses that they can attend.\n",
    "<|im_end|>\n",
    "<|im_start|>user\n",
    "{query}\n",
    "<|im_reend|>\n",
    "\"\"\"\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c837fad-d2a8-44bf-b091-7dac08ee36ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_smoll_messages(query, chunks, corpus_embedding):\n",
    "    context_str = \"\\n\\n\".join(get_context(query, chunks, corpus_embedding))\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": f\"\"\"You reply to the user's request using only context information.\n",
    "Context information to answer \"{query}\" is below\n",
    "------\n",
    "Context:\n",
    "{context_str}\n",
    "------\n",
    "You are a helpful assistant for a Computer Science university. You reply to students'questions about the courses that they can attend.\n",
    "\"\"\"},\n",
    "        {\"role\": \"user\", \"content\": query},\n",
    "    ]\n",
    "\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba8291c-c285-4711-bb9c-f0d5b771b49f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "messages = build_smoll_messages(\"Who is the NLP teacher?\", chunks, corpus_embedding)\n",
    "\n",
    "input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model_generator.generate(inputs, max_new_tokens=100, temperature=0.01, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "652d625b-19fe-40c6-9ce0-64f0679cb17d",
   "metadata": {},
   "source": [
    "# Groq generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b3e8b13-e550-493a-971f-e31ad47c3bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "groq_api_key = \"YOUR-API-KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d08068-9985-4d38-82cd-4e6a58ece916",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9fa7ce-fc3e-45eb-80d7-72b5ef06e913",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = openai.OpenAI(\n",
    "    api_key=groq_api_key,\n",
    "    base_url=\"https://api.groq.com/openai/v1\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee116f6-8847-4e1f-85b5-f22f0d3132bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What must I do to pass the NLP class?\"\n",
    "\n",
    "context_str = \"\\n\\n\".join(get_context(query, chunks, corpus_embedding))\n",
    "\n",
    "prompt = f\"\"\"Context information is below.\n",
    "---------------------\n",
    "{context_str}\n",
    "---------------------\n",
    "Given the context information and not prior knowledge, answer the query.\n",
    "If the answer is not in the context information, reply \"I cannot answer that question\".\n",
    "Query: {query}\n",
    "Answer:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9d5f32-15dd-4f27-8788-018657f8a2dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = client.chat.completions.create(                                            \n",
    "    messages=[{\"role\": \"user\", \"content\": prompt}],                              \n",
    "    model=\"openai/gpt-oss-20b\",                                                                 \n",
    ")                                                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af821664-0401-4715-adc4-fb62e46dc4d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "res.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc86e026-afa4-4a72-ae2b-1268573e12c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
